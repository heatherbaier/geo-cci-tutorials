{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee7acdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, models\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from copy import deepcopy\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a81861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acea262",
   "metadata": {},
   "source": [
    "#### Create a Transforms class. This is a set up transformations that you want to apply to each image in your dataset. The ToTensor and CenterCrop transforms are required and the Normalize transform is recommended. ALl other transforms are up to you to choose. See a list of transforms here: https://pytorch.org/vision/stable/transforms.html. Another fun site for transofmrs is here: https://imgaug.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf53e355",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfs = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.CenterCrop(256),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeae3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [1,2]\n",
    "\n",
    "if type(l) != list:\n",
    "    print('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397b218a",
   "metadata": {},
   "source": [
    "#### Create a dataloader class. This will store all of the imagery of individual schools in your dataset along with their associated test scores. It will handle splitting the data into training and validation sets and converting the sets into PyTorch's Dataloader format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed8c865",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader():\n",
    "    \n",
    "    def __init__(self, country, imagery_direc, scores_df, split, batch_size, tfs = None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            country: one of ['mex', 'slv', 'peru', 'phl']\n",
    "            imagery_direc: path to folder containing school imagery\n",
    "            scores_df: path to CSV file with school IDs and test scroes\n",
    "            split: train/test split, should be between .01 and 1, recommended is between .65 and .8\n",
    "            batch_size: number of images in a batch\n",
    "        \"\"\"\n",
    "        self.country = country\n",
    "        self.imagery_direc = imagery_direc\n",
    "        self.imagery = os.listdir(self.imagery_direc)\n",
    "        \n",
    "        if type(country) == list:\n",
    "            lsts = []\n",
    "            for c in country:\n",
    "                lsts.append([i for i in self.imagery if c in i])\n",
    "            self.imagery = [item for sublist in lsts for item in sublist]\n",
    "            self.scores_df = pd.read_csv(scores_df)\n",
    "            self.scores_df = self.scores_df[self.scores_df['country'].isin(self.country)]            \n",
    "            \n",
    "            \n",
    "        else:\n",
    "            self.imagery = [i for i in self.imagery if self.country in i]\n",
    "            self.scores_df = pd.read_csv(scores_df)\n",
    "            self.scores_df = self.scores_df[self.scores_df['country'] == self.country]        \n",
    "\n",
    "        self.split = split\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        if tfs is None:\n",
    "            self.tfs = transforms.ToTensor()\n",
    "        else:\n",
    "            self.tfs = tfs\n",
    "        \n",
    "        # Load the data into a list with the format [(school_image, school_test_score), ...]\n",
    "        self.data = self.load_data()\n",
    "        \n",
    "        # Split the data into training and validation sets\n",
    "        self.train, self.val = self.test_train_split()\n",
    "        \n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load the imagery into a list in the format: [(imager_tensor, test_score), ...]\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        for col, row in self.scores_df.iterrows():\n",
    "            school_id = str(row.school_id)\n",
    "            test_score = row.y\n",
    "            impath = [i for i in self.imagery if school_id in i]\n",
    "            if len(impath) > 0:\n",
    "                image = np.array(Image.open(self.imagery_direc + impath[0]))\n",
    "                image = self.tfs(image)\n",
    "                data.append((image, test_score))\n",
    "        return data\n",
    "                \n",
    "        \n",
    "    def test_train_split(self):\n",
    "        \"\"\"\n",
    "        Split the data into 4 parts:\n",
    "            x_train: imagery we use to train the model\n",
    "            y_train: test_scores matched to the iamgery in x_train\n",
    "            x_val: imagery used to test the model\n",
    "            y_val: test_scores matched to the iamgery in x_val\n",
    "        \"\"\"\n",
    "        train_num = int(len(self.data) * self.split)\n",
    "        val_num = int(len(self.data) - train_num)\n",
    "\n",
    "        all_indices = list(range(0, len(self.data)))\n",
    "        train_indices = random.sample(range(len(self.data)), train_num)\n",
    "        val_indices = list(np.setdiff1d(all_indices, train_indices))\n",
    "\n",
    "        x_train, x_val = [self.data[i][0] for i in train_indices], [self.data[i][0] for i in val_indices]\n",
    "        y_train, y_val = [self.data[i][1] for i in train_indices], [self.data[i][1] for i in val_indices]\n",
    "        \n",
    "        train = [(k,v) for k,v in zip(x_train, y_train)]\n",
    "        val = [(k,v) for k,v in zip(x_val, y_val)]\n",
    "\n",
    "        # Load the data into pytorch's Dataloader format\n",
    "        train = torch.utils.data.DataLoader(train, batch_size = self.batch_size, shuffle = True, drop_last = True)\n",
    "        val = torch.utils.data.DataLoader(val, batch_size = self.batch_size, shuffle = True, drop_last = True)\n",
    "\n",
    "        return train, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13383e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to train on one country, use this line\n",
    "COUNTRY = \"mex\"\n",
    "\n",
    "# If you want to train on multiple countries, comment out the line above and then use this one!\n",
    "# COUNTRY = [\"mex\", \"phl\"]\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "SPLIT = .75\n",
    "IMAGERY_DIREC = \"../../CCI/hmbaier/\"\n",
    "SCORES_DF = \"../../CCI/hmbaier/cci_example.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675ca7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataloader(country = COUNTRY, \n",
    "                  imagery_direc = IMAGERY_DIREC, \n",
    "                  scores_df = SCORES_DF,\n",
    "                  split = SPLIT,\n",
    "                  batch_size = BATCH_SIZE,\n",
    "                  tfs = tfs)\n",
    "train_dl, val_dl = data.train, data.val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcae5f8a",
   "metadata": {},
   "source": [
    "#### Plot a couple sample images from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf94409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    \n",
    "    \n",
    "train_features, train_labels = next(iter(train_dl))\n",
    "out = torchvision.utils.make_grid(train_features)\n",
    "imshow(out, title = [i.item() for i in train_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0366e5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the to keep track of our training stastics (i.e. running training loss, running validation loss, etc...)\n",
    "class AverageMeter:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n = 1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = round(self.sum / self.count, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419cb3b4",
   "metadata": {},
   "source": [
    "We're going to use a basic off the shelf Resnet18 pretrained on ImageNet. ImageNet has 1000 classes, which means our last lauyer, the fully connected layer, has an output size of 1000 (each element in the output represents and ImageNet class). In our case, we want an output size of 1, and that number is our predicted test score. We make the change in the second line using ```model.fc = torch.nn.Linear(512, 1)```. After that, we send the model to a 'device'. If your enviroenmt has a GPU avail;abel to it, this will be \"cuda\" and if not it will be \"cpu\". We save this device as a variable, because later on we'll need to send our input images and target test_score tensors to the same device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10beb913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a basic off the shelf \n",
    "model = models.resnet18(pretrained = True)\n",
    "model.fc = torch.nn.Linear(512, 2)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.001)\n",
    "epochs = 15\n",
    "dataloader = {'Training': train_dl, 'Validation': val_dl}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675ddff3",
   "metadata": {},
   "source": [
    "#### Set up our hyperparameters\n",
    "- criterion: Thi sis our loss function. See more options here: https://pytorch.org/docs/stable/nn.html#loss-functions. L1 Loss is basically the same thing as Total Error which means our averaged loss is the smae thing as Mean Average Error (MAE) - note that if you change the type of loss function, the averageed loss will no longer be the MAE\n",
    "- optimizer: this is how we actually update the weights of our model. See more options here: https://pytorch.org/docs/stable/optim.html#algorithms. --lr is the Learning Rate.\n",
    "- epochs: The number of epochs to train\n",
    "- dataloader: Dictionary to help in the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1038fe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = torch.nn.Softmax()\n",
    "\n",
    "def train(dataloader, model, epochs, optimizer, criterion, batch_size, device):\n",
    "    \n",
    "    train_tracker, val_tracker = AverageMeter(), AverageMeter()\n",
    "    trackers = {'Training': train_tracker, \"Validation\": val_tracker}\n",
    "    best_mae, best_model_wts, best_acc = 90000000, model.state_dict(), 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        print('Epoch {}/{}'.format(epoch, epochs - 1))\n",
    "        \n",
    "        for phase in [\"Training\", \"Validation\"]:\n",
    "            \n",
    "            running_corrects = 0\n",
    "            \n",
    "            for (inputs, targets) in dataloader[phase]:\n",
    "          \n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                _, preds = torch.max(sm(outputs), 1)\n",
    "\n",
    "                loss = criterion(outputs, targets)\n",
    "                trackers[phase].update(loss.item(), batch_size)\n",
    "            \n",
    "                if phase == \"Training\":\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                running_corrects += torch.sum(preds == targets.data)\n",
    "                        \n",
    "            epoch_acc = running_corrects.double() / trackers[phase].count\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, trackers[phase].avg, epoch_acc))\n",
    "        \n",
    "        \n",
    "        if epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            best_model_wts = deepcopy(model.state_dict())\n",
    "            print(\"  * New best Accuracy!\\n\")\n",
    "        else:\n",
    "            print(\"\\n\")\n",
    "        \n",
    "        trackers[\"Training\"].reset()\n",
    "        trackers[\"Validation\"].reset()\n",
    "        \n",
    "    model.load_state_dict(best_model_wts)\n",
    "        \n",
    "    return model\n",
    "\n",
    "\n",
    "model = train(dataloader, model, epochs, optimizer, criterion, 4, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d89990d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloader['Validation']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('predicted: {}; true: {}'.format(preds[j], labels[j]))\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)\n",
    "        \n",
    "visualize_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08e3ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': epochs,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': criterion,\n",
    "        }, f\"./trained_{COUNTRY}_model.torch\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
